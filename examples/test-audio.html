<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Audio & Video Encode/Decode Test</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        max-width: 900px;
        margin: 0 auto;
        padding: 20px;
        background: #f5f5f5;
      }
      h1 {
        text-align: center;
        color: #333;
      }
      .container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 20px;
        margin-bottom: 20px;
      }
      .panel {
        background: white;
        padding: 20px;
        border-radius: 8px;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      }
      .panel-title {
        font-size: 14px;
        color: #666;
        margin: 0 0 10px 0;
        text-transform: uppercase;
        letter-spacing: 1px;
      }
      .controls {
        display: flex;
        gap: 10px;
        flex-wrap: wrap;
        margin-bottom: 15px;
      }
      button {
        padding: 10px 16px;
        background: #007bff;
        color: white;
        border: none;
        border-radius: 4px;
        cursor: pointer;
        font-size: 13px;
        font-weight: 500;
      }
      button:hover {
        background: #0056b3;
      }
      button:disabled {
        background: #ccc;
        cursor: not-allowed;
      }
      button.danger {
        background: #f44336;
      }
      button.danger:hover {
        background: #d32f2f;
      }
      .status {
        padding: 10px;
        border-radius: 4px;
        margin-bottom: 10px;
        font-size: 12px;
        background: #e7f3ff;
        border-left: 4px solid #007bff;
      }
      .status.error {
        background: #ffe7e7;
        border-left-color: #d32f2f;
      }
      .status.success {
        background: #e7ffe7;
        border-left-color: #4caf50;
      }
      .stats {
        background: #f9f9f9;
        padding: 10px;
        border-radius: 4px;
        font-size: 12px;
        font-family: monospace;
        max-height: 200px;
        overflow-y: auto;
      }
      .stat-line {
        padding: 4px 0;
        border-bottom: 1px solid #eee;
        display: flex;
        justify-content: space-between;
      }
      .stat-label {
        color: #666;
      }
      .stat-value {
        color: #333;
        font-weight: bold;
      }
      h3 {
        margin: 0 0 15px 0;
        font-size: 16px;
        color: #333;
      }
      .logs-panel {
        grid-column: 1 / -1;
      }
      #logs {
        max-height: 200px;
      }
      .logs-panel .stat-line {
        display: block;
        padding: 3px 0;
        font-size: 11px;
      }
      input[type="file"] {
        margin-bottom: 10px;
        width: 100%;
      }
      audio {
        width: 100%;
        margin-bottom: 10px;
      }
      video.preview {
        width: 100%;
        max-height: 200px;
        background: #000;
        border-radius: 4px;
        margin-bottom: 10px;
      }
      canvas.output {
        width: 100%;
        max-height: 200px;
        background: #000;
        border-radius: 4px;
        margin-bottom: 10px;
        image-rendering: auto;
      }
      .full-width {
        grid-column: 1 / -1;
      }
      .source-selector {
        grid-column: 1 / -1;
        background: white;
        padding: 15px 20px;
        border-radius: 8px;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        display: flex;
        align-items: center;
        gap: 20px;
      }
      .source-selector label {
        font-size: 14px;
        font-weight: 500;
        color: #333;
        cursor: pointer;
        display: flex;
        align-items: center;
        gap: 6px;
      }
      .source-selector input[type="radio"] {
        accent-color: #007bff;
      }
      .source-selector .selector-title {
        font-size: 14px;
        color: #666;
        text-transform: uppercase;
        letter-spacing: 1px;
        margin-right: 10px;
      }
      .panel.inactive {
        opacity: 0.45;
        pointer-events: none;
      }
      button.success {
        background: #4caf50;
      }
      button.success:hover {
        background: #388e3c;
      }
    </style>
  </head>
  <body>
    <h1>Audio & Video Encode/Decode Test</h1>
    <p style="text-align: center; color: #666">
      Audio: Opus encode/decode | Video: H.264 WASM (x264 + TinyH264) encode/decode
    </p>

    <div class="container">
      <!-- Source Selector -->
      <div class="source-selector">
        <span class="selector-title">Source:</span>
        <label>
          <input type="radio" name="audioSource" value="file" checked onchange="switchSource('file')" />
          Audio File (MP3)
        </label>
        <label>
          <input type="radio" name="audioSource" value="mic" onchange="switchSource('mic')" />
          Microphone
        </label>
      </div>

      <!-- Input Audio File -->
      <div class="panel" id="filePanel">
        <div class="panel-title">Source: Audio File</div>
        <audio id="sourceAudio" controls src="/example.mp3"></audio>
        <div class="status" id="sourceStatus">Loading example.mp3...</div>
        <div class="stats">
          <div class="stat-line">
            <span class="stat-label">File:</span>
            <span class="stat-value" id="fileName">example.mp3</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Duration:</span>
            <span class="stat-value" id="fileDuration">-</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Sample Rate:</span>
            <span class="stat-value" id="fileSampleRate">-</span>
          </div>
        </div>
      </div>

      <!-- Input Microphone -->
      <div class="panel inactive" id="micPanel">
        <div class="panel-title">Source: Microphone</div>
        <div class="controls">
          <button id="btnStartMic" onclick="startMic()" class="success">ðŸŽ™ Start Mic</button>
          <button id="btnStopMic" onclick="stopMic()" class="danger" disabled>Stop Mic</button>
        </div>
        <div class="status" id="micStatus">Microphone not started</div>
        <div class="stats">
          <div class="stat-line">
            <span class="stat-label">Device:</span>
            <span class="stat-value" id="micDevice">-</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Sample Rate:</span>
            <span class="stat-value" id="micSampleRate">-</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Channels:</span>
            <span class="stat-value" id="micChannels">-</span>
          </div>
        </div>
      </div>

      <!-- Output Audio -->
      <div class="panel">
        <div class="panel-title">Output: Decoded Opus</div>
        <div class="status" id="outputStatus">Ready</div>
        <div class="controls">
          <button onclick="startEncoding()">Start Encode/Decode</button>
          <button onclick="stopEncoding()" class="danger">Stop</button>
        </div>
        <div class="stats">
          <div class="stat-line">
            <span class="stat-label">Encoded Chunks:</span>
            <span class="stat-value" id="encodedChunks">0</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Decoded Chunks:</span>
            <span class="stat-value" id="decodedChunks">0</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Buffer (ms):</span>
            <span class="stat-value" id="bufferMs">0</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Is Playing:</span>
            <span class="stat-value" id="isPlaying">no</span>
          </div>
        </div>
      </div>

      <!-- ========== VIDEO SECTION ========== -->
      <div class="panel full-width" style="border-top: 3px solid #ff9800;">
        <h3 style="color: #ff9800;">ðŸŽ¬ Video H.264 Encode/Decode (WASM)</h3>
      </div>

      <!-- Video Input -->
      <div class="panel">
        <div class="panel-title">Video Input: Camera + Mic</div>
        <video id="videoPreview" class="preview" autoplay playsinline muted></video>
        <div class="status" id="videoInputStatus">Camera not started</div>
        <div class="controls">
          <button id="btnStartVideo" onclick="startVideo()" class="success">ðŸ“¹ Start Video + Audio</button>
          <button id="btnStopVideo" onclick="stopVideo()" class="danger" disabled>Stop</button>
        </div>
        <div class="stats">
          <div class="stat-line">
            <span class="stat-label">Resolution:</span>
            <span class="stat-value" id="videoRes">-</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Capture FPS:</span>
            <span class="stat-value" id="videoFps">-</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Video Encoded:</span>
            <span class="stat-value" id="videoEncodedFrames">0</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Video Decoded:</span>
            <span class="stat-value" id="videoDecodedFrames">0</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Audio Encoded:</span>
            <span class="stat-value" id="videoAudioEncoded">0</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Audio Decoded:</span>
            <span class="stat-value" id="videoAudioDecoded">0</span>
          </div>
        </div>
      </div>

      <!-- Video Output -->
      <div class="panel">
        <div class="panel-title">Video Output: Decoded H.264 + Opus</div>
        <canvas id="videoOutput" class="output" width="640" height="480"></canvas>
        <div class="status" id="videoOutputStatus">Ready</div>
        <div class="stats">
          <div class="stat-line">
            <span class="stat-label">Video Encode:</span>
            <span class="stat-value" id="videoEncoderType">Worker (x264 WASM)</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Video Decode:</span>
            <span class="stat-value" id="videoDecoderType">Worker (TinyH264 WASM)</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Audio Codec:</span>
            <span class="stat-value">Opus (Worker)</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Video Bitrate:</span>
            <span class="stat-value" id="videoBitrate">-</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Output Size:</span>
            <span class="stat-value" id="videoOutputSize">-</span>
          </div>
          <div class="stat-line">
            <span class="stat-label">Audio Playing:</span>
            <span class="stat-value" id="videoAudioPlaying">no</span>
          </div>
        </div>
      </div>

      <!-- Logs -->
      <div class="panel logs-panel">
        <h3>Event Logs</h3>
        <div class="stats" id="logs"></div>
      </div>
    </div>

    <!-- Polyfills for Safari iOS 15 (must load before main module) -->
    <script src="/polyfills/encodedAudioChunk.js"></script>
    <script src="/polyfills/audioData.js"></script>

    <script type="module">
      // Define log function FIRST before anything else
      function log(message, isError = false) {
        console.log("[test-audio] " + message);
        const logsDiv = document.getElementById("logs");
        if (!logsDiv) return;
        const time = new Date().toLocaleTimeString();
        const entry = document.createElement("div");
        entry.className = "stat-line";
        entry.style.color = isError ? "#d32f2f" : "#333";
        entry.textContent = `[${time}] ${message}`;
        logsDiv.insertBefore(entry, logsDiv.firstChild);
        if (logsDiv.children.length > 50)
          logsDiv.removeChild(logsDiv.lastChild);
      }

      function updateStatus(element, message, isError = false, isSuccess = false) {
        const statusDiv = document.getElementById(element);
        if (!statusDiv) return;
        statusDiv.textContent = message;
        statusDiv.className = "status";
        if (isError) statusDiv.classList.add("error");
        if (isSuccess) statusDiv.classList.add("success");
        log(message, isError);
      }

      log("Script starting...");

      import { OpusAudioDecoder } from "/opus_decoder/opusDecoder.js";
      log("OpusAudioDecoder imported");

      let initAudioRecorder = null;
      let moduleLoaded = false;
      
      (async function loadModule() {
        try {
          log("Loading opusDecoder module...");
          const opusModule = await import(
            `/opus_decoder/opusDecoder.js?t=${Date.now()}`
          );
          initAudioRecorder = opusModule.initAudioRecorder;
          moduleLoaded = true;
          log("OpusStreamer module loaded: " + (initAudioRecorder ? "success" : "failed"));
        } catch (err) {
          console.error("Error loading modules:", err);
          log("Error loading modules: " + err.message, true);
        }
      })();

      let currentSource = "file"; // 'file' or 'mic'
      let micStream = null;

      let state = {
        audioFile: null,
        audioElement: null,
        audioStream: null,
        audioRecorder: null,
        opusDecoder: null,
        encodedChunks: 0,
        decodedChunks: 0,
        isEncoding: false,
      };

      let audioContext = null;
      let audioWorkletNode = null;
      let audioMessageChannel = null;

      function switchSource(source) {
        currentSource = source;
        const filePanel = document.getElementById("filePanel");
        const micPanel = document.getElementById("micPanel");
        if (source === "file") {
          filePanel.classList.remove("inactive");
          micPanel.classList.add("inactive");
        } else {
          filePanel.classList.add("inactive");
          micPanel.classList.remove("inactive");
        }
        log("Switched source to: " + source);
      }

      async function startMic() {
        try {
          updateStatus("micStatus", "Requesting microphone access...");
          micStream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              sampleRate: 48000,
            },
            video: false,
          });
          const track = micStream.getAudioTracks()[0];
          const settings = track.getSettings();
          document.getElementById("micDevice").textContent = track.label || "Default";
          document.getElementById("micSampleRate").textContent = (settings.sampleRate || 48000) + " Hz";
          document.getElementById("micChannels").textContent = settings.channelCount || 1;
          document.getElementById("btnStartMic").disabled = true;
          document.getElementById("btnStopMic").disabled = false;
          updateStatus("micStatus", "Microphone active", false, true);
        } catch (err) {
          updateStatus("micStatus", "Mic error: " + err.message, true);
          log("getUserMedia error: " + err.message, true);
        }
      }

      function stopMic() {
        if (micStream) {
          micStream.getTracks().forEach(t => t.stop());
          micStream = null;
        }
        document.getElementById("btnStartMic").disabled = false;
        document.getElementById("btnStopMic").disabled = true;
        document.getElementById("micDevice").textContent = "-";
        document.getElementById("micSampleRate").textContent = "-";
        document.getElementById("micChannels").textContent = "-";
        updateStatus("micStatus", "Microphone stopped");
      }

      // Auto-load example.mp3 when page loads
      const audioElement = document.getElementById("sourceAudio");
      state.audioElement = audioElement;
      
      audioElement.onloadedmetadata = () => {
        document.getElementById("fileDuration").textContent = 
          audioElement.duration.toFixed(2) + "s";
        updateStatus("sourceStatus", "Audio file loaded", false, true);
        state.audioFile = true; // Mark as ready
      };

      audioElement.onerror = () => {
        updateStatus("sourceStatus", "Error loading example.mp3", true);
      };

      let scriptProcessorNode = null; // Fallback for Safari 15
      let audioBufferQueue = []; // Buffer for ScriptProcessorNode

      async function initAudioSystem() {
        try {
          // AudioContext is already created and resumed in startEncoding() click handler
          // to preserve the user gesture context (required for iOS 15).
          // Only create a new one if it doesn't exist yet.
          if (!audioContext || audioContext.state === "closed") {
            audioContext = new (window.AudioContext || window.webkitAudioContext)({
              sampleRate: 48000,
            });
          }

          // Check if AudioWorklet is supported (Safari 15 doesn't support it)
          if (audioContext.audioWorklet) {
            console.warn('[Audio] Adding audio worklet module, (initAudioSystem)');
            await audioContext.audioWorklet.addModule(
              `audio-worklet.js?t=${Date.now()}`
            );
            
            audioWorkletNode = new AudioWorkletNode(
              audioContext,
              "jitter-resistant-processor"
            );

            // Táº¡o MessageChannel Ä‘á»ƒ truyá»n audio data vÃ o AudioWorklet
            audioMessageChannel = new MessageChannel();

            // Gá»­i port2 vÃ o AudioWorklet Ä‘á»ƒ nháº­n decoded audio data
            audioWorkletNode.port.postMessage(
              { type: "connectWorker", port: audioMessageChannel.port2 },
              [audioMessageChannel.port2]
            );

            // Káº¿t ná»‘i vá»›i loa Ä‘á»ƒ phÃ¡t Ã¢m thanh
            audioWorkletNode.connect(audioContext.destination);

            audioWorkletNode.port.onmessage = (event) => {
              const { type, bufferMs, isPlaying } = event.data;
              if (type === "bufferStatus") {
                document.getElementById("bufferMs").textContent = bufferMs.toFixed(0);
                document.getElementById("isPlaying").textContent = isPlaying ? "yes" : "no";
              }
            };
            
            log("âœ“ Audio system initialized (AudioWorklet)");
          } else {
            // Fallback: Use ScriptProcessorNode for Safari 15
            log("AudioWorklet not supported, using ScriptProcessorNode fallback");
            
            scriptProcessorNode = audioContext.createScriptProcessor(4096, 2, 2);
            audioBufferQueue = [];
            
            scriptProcessorNode.onaudioprocess = (e) => {
              const outputLeft = e.outputBuffer.getChannelData(0);
              const outputRight = e.outputBuffer.getChannelData(1);
              const bufferLength = outputLeft.length;
              
              // Fill from queue
              for (let i = 0; i < bufferLength; i++) {
                if (audioBufferQueue.length > 0) {
                  const sample = audioBufferQueue.shift();
                  outputLeft[i] = sample.left;
                  outputRight[i] = sample.right;
                } else {
                  outputLeft[i] = 0;
                  outputRight[i] = 0;
                }
              }
              
              // Update UI
              const bufferMs = (audioBufferQueue.length / audioContext.sampleRate) * 1000;
              document.getElementById("bufferMs").textContent = bufferMs.toFixed(0);
              document.getElementById("isPlaying").textContent = audioBufferQueue.length > 0 ? "yes" : "no";
            };
            
            scriptProcessorNode.connect(audioContext.destination);
            log("âœ“ Audio system initialized (ScriptProcessorNode fallback)");
          }

          return true;
        } catch (e) {
          log("Audio system init error: " + e.message, true);
          console.error("Audio init error:", e);
          if (audioContext && audioContext.state !== "closed") {
            audioContext.close().catch(() => {});
          }
          audioContext = null;
          return false;
        }
      }
      
      // Function to add decoded audio to playback
      function addDecodedAudioToPlayback(channelData, sampleRate, numberOfChannels) {
        if (audioWorkletNode && audioMessageChannel) {
          // Use AudioWorklet path
          audioMessageChannel.port1.postMessage({
            type: "audioData",
            channelData: channelData,
            sampleRate: sampleRate,
            numberOfChannels: numberOfChannels,
          });
        } else if (scriptProcessorNode) {
          // Use ScriptProcessorNode fallback path
          const leftChannel = channelData[0];
          const rightChannel = channelData.length > 1 ? channelData[1] : channelData[0];
          
          for (let i = 0; i < leftChannel.length; i++) {
            audioBufferQueue.push({
              left: leftChannel[i],
              right: rightChannel[i]
            });
          }
          
          // Limit buffer size
          const maxBufferSize = 48000 * 2; // 2 seconds
          if (audioBufferQueue.length > maxBufferSize) {
            audioBufferQueue.splice(0, audioBufferQueue.length - maxBufferSize);
          }
        }
      }

      async function startEncoding() {
        log("startEncoding() called (source: " + currentSource + ")");
        console.log("startEncoding() - state:", state);
        console.log("initAudioRecorder:", initAudioRecorder);
        console.log("moduleLoaded:", moduleLoaded);

        // Check if module is loaded
        if (!initAudioRecorder) {
          log("ERROR: initAudioRecorder not loaded yet!", true);
          updateStatus("outputStatus", "Module not loaded. Please wait and try again.", true);
          return;
        }

        if (currentSource === "file" && (!state.audioFile || !state.audioElement)) {
          updateStatus("outputStatus", "Please select an audio file first", true);
          return;
        }

        if (currentSource === "mic" && !micStream) {
          updateStatus("outputStatus", "Please start the microphone first", true);
          return;
        }

        try {
          // IMPORTANT: On iOS 15 Safari, BOTH AudioContext.resume() AND
          // HTMLMediaElement.play() require a user gesture context.
          // The gesture context is lost after the first await, so we must
          // trigger ALL gesture-gated APIs synchronously BEFORE any await.

          // 1) Close old context (fire-and-forget to preserve gesture)
          if (audioContext && audioContext.state !== "closed") {
            audioContext.close().catch(() => {});
          }

          // 2) Create new AudioContext and resume â€” synchronously in click handler
          audioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 48000,
          });
          const resumePromise = audioContext.resume();
          log("AudioContext created and resume() called in click handler");

          if (currentSource === "file") {
            // 3) Unlock the audio element for playback â€” synchronously in click handler.
            //    iOS requires play() in a user gesture; once unlocked, later play() calls work.
            const audioElement = state.audioElement;
            audioElement.currentTime = 0;
            const playPromise = audioElement.play();
            log("audioElement.play() called in click handler to unlock playback");

            // Wait for both resume and play unlock with timeout
            try {
              const timeoutPromise = new Promise((_, reject) =>
                setTimeout(() => reject(new Error("Resume timeout")), 3000)
              );
              await Promise.race([resumePromise, timeoutPromise]);
              log("AudioContext resumed successfully (state: " + audioContext.state + ")");
            } catch (resumeErr) {
              log("AudioContext resume error/timeout: " + resumeErr.message, true);
            }

            // Wait for play unlock, then pause â€” we just needed to unlock it
            try {
              await playPromise;
              log("audioElement unlocked for playback");
            } catch (playErr) {
              log("audioElement unlock play error: " + playErr.message, true);
            }
            audioElement.pause();
            audioElement.currentTime = 0;
          } else {
            // Mic source: just wait for resume
            try {
              const timeoutPromise = new Promise((_, reject) =>
                setTimeout(() => reject(new Error("Resume timeout")), 3000)
              );
              await Promise.race([resumePromise, timeoutPromise]);
              log("AudioContext resumed successfully (state: " + audioContext.state + ")");
            } catch (resumeErr) {
              log("AudioContext resume error/timeout: " + resumeErr.message, true);
            }
          }

          log("AudioContext state after resume: " + audioContext.state);

          log("Initializing audio system...");
          // Init audio system (pass already-resumed audioContext)
          const initSuccess = await initAudioSystem();
          log("initAudioSystem result: " + initSuccess);

          if (!initSuccess || !audioContext) {
            updateStatus("outputStatus", "Failed to initialize audio system", true);
            return;
          }

          if (currentSource === "file") {
            // === FILE SOURCE ===
            const audioElement = state.audioElement;
            log("Creating MediaElementSource...");
            const sourceNode = audioContext.createMediaElementSource(audioElement);
            log("Created sourceNode");
            
            const streamDestination = audioContext.createMediaStreamDestination();
            log("Created streamDestination");
            
            sourceNode.connect(streamDestination);
            log("Connected nodes");
            
            state.audioStream = streamDestination.stream;
            document.getElementById("fileSampleRate").textContent = audioContext.sampleRate + " Hz";
          } else {
            // === MIC SOURCE ===
            log("Using microphone stream directly");
            state.audioStream = micStream;
          }

          log("Got audio stream: " + (state.audioStream ? "yes" : "no"));

          // Init Opus Encoder
          log("Initializing Opus Encoder...");
          const audioRecorderOptions = {
            encoderApplication: 2051,
            encoderComplexity: 0,
            encoderFrameSize: 20,
            timeSlice: 100,
          };

          state.audioRecorder = await initAudioRecorder(
            state.audioStream,
            audioRecorderOptions,
            audioContext
          );
          log("Opus Encoder initialized: " + (state.audioRecorder ? "yes" : "no"));

          let audioTimestamp = 0;
          state.encodedChunks = 0;
          state.decodedChunks = 0;

          state.audioRecorder.ondataavailable = (typedArray) => {
            state.encodedChunks++;
            log("Encoded chunk " + state.encodedChunks);
            document.getElementById("encodedChunks").textContent = state.encodedChunks;

            const chunk = new EncodedAudioChunk({
              timestamp: audioTimestamp,
              type: "key",
              data: typedArray,
            });
            
            if (state.opusDecoder) {
              state.opusDecoder.decode(chunk);
            }
            
            audioTimestamp += 85333; // Increment timestamp for next chunk
          };

          // Init Opus Decoder
          log("Initializing Opus Decoder...");
          state.opusDecoder = new OpusAudioDecoder({
            output: async (audioData) => {
              state.decodedChunks++;
              log("Decoded chunk " + state.decodedChunks);
              document.getElementById("decodedChunks").textContent = state.decodedChunks;

              const channelData = [];
              for (let i = 0; i < audioData.numberOfChannels; i++) {
                const channel = new Float32Array(audioData.numberOfFrames);
                audioData.copyTo(channel, { planeIndex: i });
                channelData.push(channel);
              }

              addDecodedAudioToPlayback(channelData, audioData.sampleRate, audioData.numberOfChannels);
              audioData.close();
            },
            error: (e) => log("Opus decoder error: " + e.message, true),
          });

          log("Configuring Opus Decoder...");
          state.opusDecoder.configure();

          // Start encoder vá»›i timeout (iOS 15 cÃ³ thá»ƒ bá»‹ treo)
          log("Starting Opus Encoder...");
          try {
            const startPromise = state.audioRecorder.start({
              timeSlice: audioRecorderOptions.timeSlice,
            });
            const startTimeout = new Promise((_, reject) => 
              setTimeout(() => reject(new Error("Recorder start timeout")), 5000)
            );
            await Promise.race([startPromise, startTimeout]);
            log("Opus Encoder started");
          } catch (startErr) {
            log("Opus Encoder start error/timeout: " + startErr.message, true);
            updateStatus("outputStatus", "Failed to start encoder. iOS may require HTTPS.", true);
            return;
          }

          if (currentSource === "file") {
            // Play audio source (element was already unlocked in click handler above)
            const audioElement = state.audioElement;
            log("Playing audio source...");
            audioElement.currentTime = 0;
            try {
              await audioElement.play();
              log("Audio playing");
            } catch (playErr) {
              log("Audio play error: " + playErr.message, true);
            }

            // Stop when audio ends
            audioElement.onended = () => {
              stopEncoding();
              log("âœ“ Audio playback completed");
            };
          } else {
            log("ðŸŽ™ Encoding from microphone â€” press Stop to end");
          }

          state.isEncoding = true;
          updateStatus("outputStatus", "Encoding/Decoding started (" + currentSource + ")", false, true);

        } catch (error) {
          updateStatus("outputStatus", "Error: " + error.message, true);
          console.error(error);
        }
      }

      async function stopEncoding() {
        state.isEncoding = false;

        if (state.audioRecorder) {
          try {
            await state.audioRecorder.stop();
          } catch (e) {}
          state.audioRecorder = null;
        }

        if (state.audioElement) {
          state.audioElement.pause();
        }

        if (audioContext && audioContext.state !== "closed") {
          await audioContext.close();
          audioContext = null;
        }

        updateStatus("outputStatus", "Stopped");
      }

      log("Ready - select an audio source to start");
      window.startEncoding = startEncoding;
      window.stopEncoding = stopEncoding;
      window.switchSource = switchSource;
      window.startMic = startMic;
      window.stopMic = stopMic;

      // ============================================================
      // VIDEO H.264 (Worker) + AUDIO OPUS (Worker) SECTION
      // ============================================================

      // â”€â”€ iOS Detection â”€â”€
      const isIOS = /iPad|iPhone|iPod/.test(navigator.userAgent) ||
                    (navigator.platform === 'MacIntel' && navigator.maxTouchPoints > 1);
      if (isIOS) {
        log('âš ï¸ iOS detected â€” applying performance optimizations (lower res, fps, bitrate)');
      }

      // iOS-adaptive defaults
      // const DEFAULT_WIDTH = isIOS ? 320 : 640;
      // const DEFAULT_HEIGHT = isIOS ? 240 : 480;
      // const DEFAULT_FPS = isIOS ? 10 : 15;
      // const DEFAULT_BITRATE = isIOS ? 250_000 : 500_000;
      // const DEFAULT_OPUS_TIMESLICE = isIOS ? 200 : 100;
      const DEFAULT_WIDTH = 640;
      const DEFAULT_HEIGHT = 360;
      const DEFAULT_FPS = 30;
      const DEFAULT_BITRATE = 250_000;
      const DEFAULT_OPUS_TIMESLICE = 100;
      
      let videoState = {
        cameraStream: null,
        // Video workers
        encoderWorker: null,
        decoderWorker: null,
        decoderPort: null,
        encoderReady: false,
        decoderReady: false,
        // Audio
        videoAudioCtx: null,
        videoAudioWorklet: null,
        videoAudioMsgChannel: null,
        videoScriptProcessor: null,
        videoAudioBufferQueue: [],
        audioRecorder: null,
        opusDecoder: null,
        audioEncodedChunks: 0,
        audioDecodedChunks: 0,
        // Video capture
        captureAnimFrame: null,
        captureCanvas: null,
        captureCtx: null,
        outputCanvas: null,
        outputCtx: null,
        isRunning: false,
        encodedFrames: 0,
        decodedFrames: 0,
        totalEncodedBytes: 0,
        bitrateStartTime: 0,
        captureWidth: DEFAULT_WIDTH,
        captureHeight: DEFAULT_HEIGHT,
        targetFps: DEFAULT_FPS,
        lastCaptureTime: 0,
      };

      const VIDEO_ENCODER_NAME = "test-video";
      const VIDEO_CHANNEL_NAME = "test-video";

      // â”€â”€ Video Audio Playback System â”€â”€
      async function initVideoAudioSystem() {
        if (!videoState.videoAudioCtx || videoState.videoAudioCtx.state === "closed") {
          videoState.videoAudioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 48000 });
        }
        try {
          await videoState.videoAudioCtx.resume();
        } catch (e) {
          log("[Video-Audio] AudioContext resume failed: " + e.message, true);
        }

        if (videoState.videoAudioCtx.audioWorklet) {
          console.warn('[Video-Audio] Adding audio worklet module, (initVideoAudioSystem)');
          await videoState.videoAudioCtx.audioWorklet.addModule(`/audio-worklet.js?t=${Date.now()}`);
          videoState.videoAudioWorklet = new AudioWorkletNode(videoState.videoAudioCtx, "jitter-resistant-processor");
          videoState.videoAudioMsgChannel = new MessageChannel();
          videoState.videoAudioMsgChannel.port1.onmessage = (event) => {
            console.warn('[Video-Audio] Message from worklet:', event.data);
          };
          videoState.videoAudioWorklet.port.postMessage(
            { type: "connectWorker", port: videoState.videoAudioMsgChannel.port2 },
            [videoState.videoAudioMsgChannel.port2]
          );
         
          videoState.videoAudioWorklet.connect(videoState.videoAudioCtx.destination);
          videoState.videoAudioWorklet.port.onmessage = (event) => {
            if (event.data.type === "bufferStatus") {
              document.getElementById("videoAudioPlaying").textContent = event.data.isPlaying ? "yes" : "no";
            }
          };
          log("[Video-Audio] âœ“ AudioWorklet initialized");
        } else {
          // ScriptProcessorNode fallback
          videoState.videoScriptProcessor = videoState.videoAudioCtx.createScriptProcessor(4096, 2, 2);
          videoState.videoAudioBufferQueue = [];
          videoState.videoScriptProcessor.onaudioprocess = (e) => {
            const outL = e.outputBuffer.getChannelData(0);
            const outR = e.outputBuffer.getChannelData(1);
            for (let i = 0; i < outL.length; i++) {
              if (videoState.videoAudioBufferQueue.length > 0) {
                const s = videoState.videoAudioBufferQueue.shift();
                outL[i] = s.left; outR[i] = s.right;
              } else { outL[i] = 0; outR[i] = 0; }
            }
            document.getElementById("videoAudioPlaying").textContent = videoState.videoAudioBufferQueue.length > 0 ? "yes" : "no";
          };
          videoState.videoScriptProcessor.connect(videoState.videoAudioCtx.destination);
          log("[Video-Audio] âœ“ ScriptProcessorNode fallback initialized");
        }
      }

      function feedDecodedAudioToPlayback(channelData, sampleRate, numberOfChannels) {
        if (videoState.videoAudioWorklet && videoState.videoAudioMsgChannel) {
          videoState.videoAudioMsgChannel.port1.postMessage({
            type: "audioData", channelData, sampleRate, numberOfChannels,
          });
        } else if (videoState.videoScriptProcessor) {
          const left = channelData[0];
          const right = channelData.length > 1 ? channelData[1] : channelData[0];
          for (let i = 0; i < left.length; i++) {
            videoState.videoAudioBufferQueue.push({ left: left[i], right: right[i] });
          }
          if (videoState.videoAudioBufferQueue.length > 48000 * 2) {
            videoState.videoAudioBufferQueue.splice(0, videoState.videoAudioBufferQueue.length - 48000 * 2);
          }
        }
      }

      // â”€â”€ Start Video + Audio Pipeline â”€â”€
      async function startVideo() {
        log("[Video] Starting video + audio pipeline...");
        try {
          // 1. Get camera + mic stream
          updateStatus("videoInputStatus", "Requesting camera + mic...");

          // Create AudioContext first (user gesture context)
          videoState.videoAudioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 48000 });
          const resumePromise = videoState.videoAudioCtx.resume();

          videoState.cameraStream = await navigator.mediaDevices.getUserMedia({
            video: { width: { ideal: DEFAULT_WIDTH }, height: { ideal: DEFAULT_HEIGHT }, frameRate: { ideal: DEFAULT_FPS } },
            audio: { echoCancellation: true, noiseSuppression: true, sampleRate: 48000 },
          });

          await resumePromise.catch(() => {});

          const videoTrack = videoState.cameraStream.getVideoTracks()[0];
          const vSettings = videoTrack.getSettings();
          console.log("[Video] Camera settings:", vSettings);
          videoState.captureWidth = Math.min(vSettings.width || DEFAULT_WIDTH, DEFAULT_WIDTH);
          videoState.captureHeight = Math.min(vSettings.height || DEFAULT_HEIGHT, DEFAULT_HEIGHT);
          videoState.targetFps = Math.min(vSettings.frameRate || DEFAULT_FPS, DEFAULT_FPS);

          // Show preview (muted to avoid echo)
          const preview = document.getElementById("videoPreview");
          preview.srcObject = videoState.cameraStream;

          document.getElementById("videoRes").textContent = `${videoState.captureWidth}x${videoState.captureHeight}`;
          document.getElementById("videoFps").textContent = videoState.targetFps + " fps";
          updateStatus("videoInputStatus", "Camera + Mic active", false, true);
          log(`[Video] Camera: ${videoState.captureWidth}x${videoState.captureHeight} @ ${videoState.targetFps}fps`);

          // 2. Setup canvases
          videoState.captureCanvas = document.createElement("canvas");
          videoState.captureCanvas.width = videoState.captureWidth;
          videoState.captureCanvas.height = videoState.captureHeight;
          videoState.captureCtx = videoState.captureCanvas.getContext("2d", { willReadFrequently: true });

          videoState.outputCanvas = document.getElementById("videoOutput");
          videoState.outputCanvas.width = videoState.captureWidth;
          videoState.outputCanvas.height = videoState.captureHeight;
          videoState.outputCtx = videoState.outputCanvas.getContext("2d", { willReadFrequently: true });

          // 3. Init H.264 Encoder Worker
          log("[Video] Starting H.264 encoder worker...");
          updateStatus("videoOutputStatus", "Loading H.264 WASM workers...");
          videoState.encoderWorker = new Worker("/workers/video-encoder-worker.js", { type: "module" });

          await new Promise((resolve, reject) => {
            const timeout = setTimeout(() => reject(new Error("Encoder worker timeout")), 10000);
            videoState.encoderWorker.onmessage = (e) => {
              if (e.data.type === "ready") { clearTimeout(timeout); resolve(); }
            };
            videoState.encoderWorker.onerror = (e) => { clearTimeout(timeout); reject(e); };
          });
          log("[Video] âœ“ Encoder worker ready");

          // 4. Init H.264 Decoder Worker
          log("[Video] Starting H.264 decoder worker...");
          videoState.decoderWorker = new Worker("/workers/video-decoder-worker.js", { type: "module" });
          const decoderChannel = new MessageChannel();
          videoState.decoderPort = decoderChannel.port1;

          await new Promise((resolve, reject) => {
            const timeout = setTimeout(() => reject(new Error("Decoder worker timeout")), 10000);
            videoState.decoderPort.onmessage = (e) => {
              if (e.data.type === "ready") { clearTimeout(timeout); resolve(); }
              else { handleDecoderMessage(e.data); }
            };
            videoState.decoderWorker.postMessage(
              { type: "init", port: decoderChannel.port2 },
              [decoderChannel.port2]
            );
          });
          log("[Video] âœ“ Decoder worker ready");

          // 5. Configure encoder worker
          await new Promise((resolve, reject) => {
            const timeout = setTimeout(() => reject(new Error("Encoder configure timeout")), 15000);
            const handler = (e) => {
              if (e.data.type === "configured") { clearTimeout(timeout); resolve(); }
              else if (e.data.type === "error") { clearTimeout(timeout); reject(new Error(e.data.message)); }
            };
            videoState.encoderWorker.addEventListener("message", handler);
            videoState.encoderWorker.postMessage({
              type: "configure",
              encoderName: VIDEO_ENCODER_NAME,
              config: {
                width: videoState.captureWidth,
                height: videoState.captureHeight,
                framerate: videoState.targetFps,
                bitrate: DEFAULT_BITRATE,
                keyFrameInterval: videoState.targetFps * 2,
              },
            });
          });
          log("[Video] âœ“ Encoder configured (Worker)");

          // Wire encoder worker output â†’ decoder worker
          videoState.encoderWorker.onmessage = (e) => {
            const data = e.data;
            if (data.type === "output") {
              videoState.encodedFrames++;
              document.getElementById("videoEncodedFrames").textContent = videoState.encodedFrames;
              if (data.chunk?.data) {
                videoState.totalEncodedBytes += data.chunk.data.byteLength || 0;
              }
              // Forward to decoder worker
              if (videoState.decoderPort) {
                videoState.decoderPort.postMessage({
                  type: "decode",
                  channelName: VIDEO_CHANNEL_NAME,
                  chunk: data.chunk,
                });
              }
            } else if (data.type === "error") {
              log("[Video] Encoder worker error: " + data.message, true);
            }
          };

          // 6. Configure decoder worker
          await new Promise((resolve, reject) => {
            const timeout = setTimeout(() => reject(new Error("Decoder configure timeout")), 15000);
            const origHandler = videoState.decoderPort.onmessage;
            videoState.decoderPort.onmessage = (e) => {
              if (e.data.type === "configured") {
                clearTimeout(timeout);
                videoState.decoderPort.onmessage = (ev) => handleDecoderMessage(ev.data);
                resolve();
              } else if (e.data.type === "error") {
                clearTimeout(timeout); reject(new Error(e.data.message));
              } else {
                handleDecoderMessage(e.data);
              }
            };
            videoState.decoderPort.postMessage({
              type: "configure",
              channelName: VIDEO_CHANNEL_NAME,
              config: { codec: "avc1.42001f" },
            });
          });
          log("[Video] âœ“ Decoder configured (Worker)");

          // 7. Init Audio System (AudioWorklet playback)
          log("[Video-Audio] Initializing audio playback...");
          await initVideoAudioSystem();

          // 8. Init Opus Encoder (worker-based via initAudioRecorder)
          log("[Video-Audio] Initializing Opus encoder...");
          const audioStream = new MediaStream(videoState.cameraStream.getAudioTracks());
          const audioRecorderOpts = {
            encoderApplication: 2051,
            encoderComplexity: 0,
            encoderFrameSize: 20,
            timeSlice: DEFAULT_OPUS_TIMESLICE,
          };
          videoState.audioRecorder = await initAudioRecorder(
            audioStream, audioRecorderOpts, videoState.videoAudioCtx
          );
          log("[Video-Audio] âœ“ Opus encoder initialized");

          // 9. Init Opus Decoder (worker-based)
          log("[Video-Audio] Initializing Opus decoder...");
          videoState.audioEncodedChunks = 0;
          videoState.audioDecodedChunks = 0;
          let audioTimestamp = 0;

          videoState.audioRecorder.ondataavailable = (typedArray) => {
            videoState.audioEncodedChunks++;
            document.getElementById("videoAudioEncoded").textContent = videoState.audioEncodedChunks;
            const chunk = new EncodedAudioChunk({
              timestamp: audioTimestamp,
              type: "key",
              data: typedArray,
            });
            if (videoState.opusDecoder) {
              videoState.opusDecoder.decode(chunk);
            }
            audioTimestamp += 85333;
          };

          videoState.opusDecoder = new OpusAudioDecoder({
            output: async (audioData) => {
              videoState.audioDecodedChunks++;
              document.getElementById("videoAudioDecoded").textContent = videoState.audioDecodedChunks;
              const channelData = [];
              for (let i = 0; i < audioData.numberOfChannels; i++) {
                const ch = new Float32Array(audioData.numberOfFrames);
                audioData.copyTo(ch, { planeIndex: i });
                channelData.push(ch);
              }
              feedDecodedAudioToPlayback(channelData, audioData.sampleRate, audioData.numberOfChannels);
              audioData.close();
            },
            error: (e) => log("[Video-Audio] Opus decoder error: " + e.message, true),
          });
          videoState.opusDecoder.configure();
          log("[Video-Audio] âœ“ Opus decoder initialized");

          // 10. Start Opus Encoder
          try {
            const startPromise = videoState.audioRecorder.start({ timeSlice: audioRecorderOpts.timeSlice });
            const startTimeout = new Promise((_, rej) => setTimeout(() => rej(new Error("Recorder start timeout")), 5000));
            await Promise.race([startPromise, startTimeout]);
            log("[Video-Audio] âœ“ Opus encoder started");
          } catch (err) {
            log("[Video-Audio] Opus start error: " + err.message, true);
          }

          // 11. Start video capture loop (requestAnimationFrame with frame-rate throttle)
          videoState.isRunning = true;
          videoState.encodedFrames = 0;
          videoState.decodedFrames = 0;
          videoState.totalEncodedBytes = 0;
          videoState.bitrateStartTime = performance.now();
          videoState.lastCaptureTime = 0;

          const frameInterval = 1000 / videoState.targetFps;
          function captureLoop(timestamp) {
            if (!videoState.isRunning) return;
            // Throttle to target FPS
            if (timestamp - videoState.lastCaptureTime >= frameInterval) {
              videoState.lastCaptureTime = timestamp;
              captureAndEncode();
            }
            videoState.captureAnimFrame = requestAnimationFrame(captureLoop);
          }
          videoState.captureAnimFrame = requestAnimationFrame(captureLoop);

          videoState.bitrateInterval = setInterval(() => {
            const elapsed = (performance.now() - videoState.bitrateStartTime) / 1000;
            if (elapsed > 0) {
              const kbps = (videoState.totalEncodedBytes * 8) / elapsed / 1000;
              document.getElementById("videoBitrate").textContent = kbps.toFixed(0) + " kbps";
            }
          }, 1000);

          updateStatus("videoOutputStatus", `Video + Audio active${isIOS ? ' (iOS optimized)' : ''}`, false, true);
          document.getElementById("btnStartVideo").disabled = true;
          document.getElementById("btnStopVideo").disabled = false;
          log(`[Video] âœ“ Full pipeline started â€” ${videoState.captureWidth}x${videoState.captureHeight}@${videoState.targetFps}fps, bitrate=${DEFAULT_BITRATE/1000}kbps, opusSlice=${DEFAULT_OPUS_TIMESLICE}ms`);

        } catch (err) {
          log("[Video] Start error: " + err.message, true);
          updateStatus("videoInputStatus", "Error: " + err.message, true);
          console.error(err);
        }
      }

      // â”€â”€ Decoder Worker Message Handler â”€â”€
      function handleDecoderMessage(data) {
        if (data.type === "videoData") {
          videoState.decodedFrames++;
          document.getElementById("videoDecodedFrames").textContent = videoState.decodedFrames;
          const frame = data.frame;
          if (frame) {
            document.getElementById("videoOutputSize").textContent = `${frame.width}x${frame.height}`;
            renderYUVToCanvas(frame);
          }
        } else if (data.type === "error") {
          log("[Video] Decoder worker error: " + data.message, true);
        }
      }

      // â”€â”€ Capture & Send to Encoder Worker â”€â”€
      function captureAndEncode() {
        if (!videoState.isRunning || !videoState.encoderWorker) return;
        const preview = document.getElementById("videoPreview");
        if (preview.readyState < 2) return;

        const ctx = videoState.captureCtx;
        const w = videoState.captureWidth;
        const h = videoState.captureHeight;

        ctx.drawImage(preview, 0, 0, w, h);
        const imageData = ctx.getImageData(0, 0, w, h);
        const rgbaBuffer = imageData.data.buffer.slice(0); // copy for transfer

        const forceKeyFrame = videoState.encodedFrames % (videoState.targetFps * 2) === 0;
        videoState.encoderWorker.postMessage(
          { type: "encode", encoderName: VIDEO_ENCODER_NAME, rgbaData: rgbaBuffer, width: w, height: h, forceKeyFrame },
          [rgbaBuffer]
        );
      }

      // â”€â”€ Render decoded YUV420 to Canvas â”€â”€
      function renderYUVToCanvas(frame) {
        if (!videoState.outputCtx) return;
        const { width, height } = frame;
        const canvas = videoState.outputCanvas;
        if (canvas.width !== width || canvas.height !== height) {
          canvas.width = width; canvas.height = height;
        }
        if (frame.yPlane && frame.uPlane && frame.vPlane) {
          const rgba = convertYUV420toRGBA(frame.yPlane, frame.uPlane, frame.vPlane, width, height);
          const imgData = new ImageData(new Uint8ClampedArray(rgba.buffer), width, height);
          videoState.outputCtx.putImageData(imgData, 0, 0);
        }
      }

      function convertYUV420toRGBA(yPlane, uPlane, vPlane, width, height) {
        const rgba = new Uint8Array(width * height * 4);
        const uvWidth = width >> 1;
        for (let j = 0; j < height; j++) {
          for (let i = 0; i < width; i++) {
            const yIdx = j * width + i;
            const uvIdx = (j >> 1) * uvWidth + (i >> 1);
            const y = yPlane[yIdx];
            const u = uPlane[uvIdx] - 128;
            const v = vPlane[uvIdx] - 128;
            let r = y + 1.402 * v;
            let g = y - 0.344136 * u - 0.714136 * v;
            let b = y + 1.772 * u;
            const px = yIdx * 4;
            rgba[px]     = r < 0 ? 0 : r > 255 ? 255 : r;
            rgba[px + 1] = g < 0 ? 0 : g > 255 ? 255 : g;
            rgba[px + 2] = b < 0 ? 0 : b > 255 ? 255 : b;
            rgba[px + 3] = 255;
          }
        }
        return rgba;
      }

      // â”€â”€ Stop Video + Audio Pipeline â”€â”€
      async function stopVideo() {
        log("[Video] Stopping full pipeline...");
        videoState.isRunning = false;

        if (videoState.captureAnimFrame) { cancelAnimationFrame(videoState.captureAnimFrame); videoState.captureAnimFrame = null; }
        if (videoState.bitrateInterval) { clearInterval(videoState.bitrateInterval); videoState.bitrateInterval = null; }

        // Stop audio encoder
        if (videoState.audioRecorder) {
          try { await videoState.audioRecorder.stop(); } catch (e) {}
          videoState.audioRecorder = null;
        }

        // Close opus decoder
        if (videoState.opusDecoder) { videoState.opusDecoder = null; }

        // Close encoder worker
        if (videoState.encoderWorker) {
          videoState.encoderWorker.postMessage({ type: "closeAll" });
          videoState.encoderWorker.terminate();
          videoState.encoderWorker = null;
        }

        // Close decoder worker
        if (videoState.decoderPort) {
          videoState.decoderPort.postMessage({ type: "resetAll" });
          videoState.decoderPort.close();
          videoState.decoderPort = null;
        }
        if (videoState.decoderWorker) {
          videoState.decoderWorker.terminate();
          videoState.decoderWorker = null;
        }

        // Close audio system
        if (videoState.videoAudioWorklet) { videoState.videoAudioWorklet.disconnect(); videoState.videoAudioWorklet = null; }
        if (videoState.videoScriptProcessor) { videoState.videoScriptProcessor.disconnect(); videoState.videoScriptProcessor = null; }
        if (videoState.videoAudioMsgChannel) { videoState.videoAudioMsgChannel = null; }
        if (videoState.videoAudioCtx && videoState.videoAudioCtx.state !== "closed") {
          await videoState.videoAudioCtx.close().catch(() => {});
          videoState.videoAudioCtx = null;
        }

        // Stop camera/mic tracks
        if (videoState.cameraStream) {
          videoState.cameraStream.getTracks().forEach(t => t.stop());
          videoState.cameraStream = null;
        }

        document.getElementById("videoPreview").srcObject = null;
        document.getElementById("btnStartVideo").disabled = false;
        document.getElementById("btnStopVideo").disabled = true;
        document.getElementById("videoRes").textContent = "-";
        document.getElementById("videoFps").textContent = "-";
        document.getElementById("videoBitrate").textContent = "-";
        document.getElementById("videoOutputSize").textContent = "-";
        document.getElementById("videoAudioPlaying").textContent = "no";

        updateStatus("videoInputStatus", "Stopped");
        updateStatus("videoOutputStatus", "Stopped");
        log("[Video] âœ“ Full pipeline stopped");
      }

      window.startVideo = startVideo;
      window.stopVideo = stopVideo;
    </script>
  </body>
</html>
